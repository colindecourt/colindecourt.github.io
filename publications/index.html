<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Publications - Colin Decourt</title><meta name=Description content="This is my cool site"><meta property="og:title" content="Publications">
<meta property="og:description" content="Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin
aXiv preprint:2402.08427
Abstract In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments."><meta property="og:type" content="article"><meta property="og:url" content="https://colindecourt.github.io/publications/"><meta property="article:section" content><meta property="article:published_time" content="2022-04-24T16:54:43+02:00"><meta property="article:modified_time" content="2022-04-24T16:54:43+02:00"><meta property="og:site_name" content="My cool site"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin
aXiv preprint:2402.08427
Abstract In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments."><meta name=application-name content="Colin Decourt"><meta name=apple-mobile-web-app-title content="Colin Decourt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://colindecourt.github.io/publications/><link rel=next href=https://colindecourt.github.io/teaching/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Publications","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/colindecourt.github.io\/publications\/"},"genre":"page","wordcount":984,"url":"https:\/\/colindecourt.github.io\/publications\/","datePublished":"2022-04-24T16:54:43+02:00","dateModified":"2022-04-24T16:54:43+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Colin Decourt"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Colin Decourt"></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>About me </a><a class=menu-item href=/publications>Publications </a><a class=menu-item href=/teaching>Teaching </a><a class=menu-item href=/pdf/cv_2024.pdf>CV </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Colin Decourt"></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/ title>About me</a><a class=menu-item href=/publications title>Publications</a><a class=menu-item href=/teaching title>Teaching</a><a class=menu-item href=/pdf/cv_2024.pdf title>CV</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class="page single special"><h1 class="single-title animate__animated animate__pulse animate__faster">Publications</h1><div class=content id=content><h3 id=leveraging-self-supervised-instance-contrastive-learning-for-radar-object-detection>Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection</h3><p><em>Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin</em><br><em>aXiv preprint:2402.08427</em></p><div class="details admonition abstract"><div class="details-summary admonition-title"><i class="icon fas fa-list-ul fa-fw" aria-hidden=true></i>Abstract<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors. Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors. We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning. We aim to pre-train an object detector&rsquo;s backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar <a href=mailto:mAP@0.5 rel>mAP@0.5</a> than a supervised approach using the whole training set.</div></div></div><p><a href=https://arxiv.org/abs/2402.08427 target=_blank rel="noopener noreffer">arXiv version</a></p><h3 id=a-recurrent-cnn-for-online-object-detection-on-raw-radar-frames>A recurrent CNN for online object detection on raw radar frames</h3><p><em>Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin</em><br><em>arXiv preprint:2212.11172</em></p><div class="details admonition abstract"><div class="details-summary admonition-title"><i class="icon fas fa-list-ul fa-fw" aria-hidden=true></i>Abstract<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Automotive radar sensors provide valuable information for advanced driving assistance systems (ADAS). Radars can reliably estimate the distance to an object and the relative velocity, regardless of weather and light conditions. However, radar sensors suffer from low resolution and huge intra-class variations in the shape of objects. Exploiting the time information (e.g., multiple frames) has been shown to help to capture better the dynamics of objects and, therefore, the variation in the shape of objects. Most temporal radar object detectors use 3D convolutions to learn spatial and temporal information. However, these methods are often non-causal and unsuitable for real-time applications. This work presents RECORD, a new recurrent CNN architecture for online radar object detection. We propose an end-to-end trainable architecture mixing convolutions and ConvLSTMs to learn spatio-temporal dependencies between successive frames. Our model is causal and requires only the past information encoded in the memory of the ConvLSTMs to detect objects. Our experiments show such a method&rsquo;s relevance for detecting objects in different radar representations (range-Doppler, range-angle) and outperform state-of-the-art models on the ROD2021 and CARRADA datasets while being less computationally expensive. The code is available <a href=https://github.com/colindecourt/record target=_blank rel="noopener noreffer">here</a>.</div></div></div><p><a href=https://arxiv.org/abs/2212.11172v2 target=_blank rel="noopener noreffer">arXiv version</a> &mdash; <a href=https://github.com/colindecourt/record target=_blank rel="noopener noreffer">Code</a></p><h3 id=darod-a-deep-automotive-radar-object-detector-on-range-doppler-maps>DAROD: A Deep Automotive Radar Object Detector on Range-Doppler maps</h3><p><em>Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin</em><br><em>IEEE Intelligent Vehicles Symposium 2022</em></p><div class="details admonition abstract"><div class="details-summary admonition-title"><i class="icon fas fa-list-ul fa-fw" aria-hidden=true></i>Abstract<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Due to the small number of raw data automotive radar datasets and the low resolution of such radar sensors, automotive radar object detection has been little explored with deep learning models in comparison to camera and lidar-based approaches. However, radars are low-cost sensors able to accurately sense surrounding object characteristics (e.g., distance, radial velocity, direction of arrival, radar cross-section) regardless of weather conditions (e.g., rain, snow, fog). Recent open-source datasets such as CARRADA, RADDet or CRUW have opened up research on several topics ranging from object classification to object detection and segmentation. In this paper, we present DAROD, an adaptation of Faster R-CNN object detector for automotive radar on the range-Doppler spectra. We propose a light architecture for features extraction, which shows an increased performance compare to heavier vision-based backbone architectures. Our models reach respectively an mAP$@$0.5 of 55.83 and 46.57 on CARRADA and RADDet datasets, outperforming competing methods.</div></div></div><p><a href=https://ieeexplore.ieee.org/document/9827281 target=_blank rel="noopener noreffer">Paper</a> &mdash; <a href=https://github.com/colindecourt/darod target=_blank rel="noopener noreffer">Code</a> &mdash; <a href=/pdf/poster_IV22_decourt_darod.pdf rel>Poster (.pdf)</a></p><h3 id=semi-supervised-generative-adversarial-networks-for-the-segmentation-of-the-left-ventricle-in-pediatric-mri>Semi-supervised generative adversarial networks for the segmentation of the left ventricle in pediatric MRI</h3><p><em>Colin Decourt, Luc Duong</em><br><em>Computers in Biology and Medicine, Volume 123, 2020</em></p><div class="details admonition abstract"><div class="details-summary admonition-title"><i class="icon fas fa-list-ul fa-fw" aria-hidden=true></i>Abstract<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Segmentation of the left ventricle in magnetic resonance imaging (MRI) is important for assessing cardiac function. We present DT-GAN, a generative adversarial network (GAN) segmentation approach for the identification of the left ventricle in pediatric MRI. Segmentation of the left ventricle requires a large amount of annotated data; generating such data can be time-consuming and subject to observer variability. Additionally, it can be difficult to accomplish in a clinical setting. During the training of our GAN, we therefore introduce a semi-supervised semantic segmentation to reduce the number of images required for training, while maintaining a good segmentation accuracy. The GAN generator produces a segmentation label map and its discriminator outputs a confidence map, which gives the probability of a pixel coming from the label or from the generator. Moreover, we propose a new formulation of the GAN loss function based on distance transform and pixel-wise cross-entropy. This new loss function provides a better segmentation of boundary pixels, by favoring the correct classification of those pixels rather than focusing on pixels that are farther away from the boundary between anatomical structures. Our proposed method achieves a mean Hausdorff distance of 2.16 mm $\pm$ 0.42 mm (2.28 mm $\pm$ 0.21 mm for U-Net) and a Dice score of 0.88 $\pm$ 0.08 (0.91 $\pm$ 0.12 for U-Net) for the endocardium segmentation, using 50% of the annotated data. For the epicardium segmentation, we achieve a mean Hausdorff distance of 2.23 mm $\pm$ 0.35 mm (2.34 mm $\pm$ 0.39 mm for U-Net) and a Dice score of 0.93 mm $\pm$ 0.04 mm (0.89 $\pm$ 0.09 for U-Net). For the myocardium segmentation, we achieve a mean Hausdorff distance of 2.98 mm $\pm$ 0.43 mm (3.04 mm $\pm$ 0.27 mm for U-Net) and a Dice score of 0.79 mm $\pm$ 0.10 mm (0.74 $\pm$ 0.04 for U-Net). This new model could be very useful for the automatic analysis of cardiac MRI and for conducting large-scale studies based on MRI readings, with a limited amount of training data.</div></div></div><p><a href=https://doi.org/10.1016/j.compbiomed.2020.103884 target=_blank rel="noopener noreffer">Journal version</a></p></div></div></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Colin Decourt</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>